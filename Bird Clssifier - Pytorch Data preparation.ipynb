{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt9udUwbRwVH"
   },
   "source": [
    "## Import and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10747,
     "status": "ok",
     "timestamp": 1707123172553,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "4ReO7EaW8a0g"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import math\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Audio\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1707123261156,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "sOl-wlAE9vLF"
   },
   "outputs": [],
   "source": [
    "def envelope(y, rate, threshold, w = 20): # this is the filter we use to remove the Silence from the audio\n",
    "  ''' this filter normalizes the values of the amplituted and filters our all pathces of signal whre the mean is bellow the set thresshold.\n",
    "the window patch depends on w in the following rate patch window = sample rate/ w\n",
    "  '''\n",
    "\n",
    "  mask = []\n",
    "  y = pd.Series(y).apply(np.abs)\n",
    "  y_mean = y.rolling(window=int(rate/w),\n",
    "                       min_periods=1,\n",
    "                       center=True).max()\n",
    "\n",
    "  for mean in y_mean:\n",
    "    if mean > threshold:\n",
    "       mask.append(True)\n",
    "    else:\n",
    "      mask.append(False)\n",
    "  return mask, y_mean\n",
    "\n",
    "def test(l1,threshold=0.1,w = 10):\n",
    "  mask, y_mean = envelope(l1[0], 16000, threshold=threshold,w = w)\n",
    "  wav = l1[0]\n",
    "  print('Sample rate points in this file:',len(wav))\n",
    "  print('Listen to original file')\n",
    "  write('test0.wav', 16000, wav)\n",
    "  display(Audio(data = 'test0.wav', rate = 16000))\n",
    "\n",
    "  fig, ax = plt.subplots(2,2, figsize=(20, 8))\n",
    "  librosa.display.waveshow(wav, ax=ax[0,0], sr = 16000)\n",
    "  librosa.display.waveshow(wav[mask], ax = ax[1,0],sr = 16000)\n",
    "  ax[0,0].set_title('Original file')\n",
    "  ax[0,0].set_ylabel('Amplitude (norm)')\n",
    "  ax[0,0].set_xlabel('Time (seconds)')\n",
    "  ax[1,0].set_title('No sielence file')\n",
    "  ax[1,0].set_xlabel('Time (seconds)')\n",
    "  ax[1,0].set_ylabel('Amplitude (norm)')\n",
    "  # ax[2].style.use('ggplot')\n",
    "  ax[0,1].plot(wav*np.array(np.logical_not(mask)), color='r', label='remove')\n",
    "  ax[0,1].plot(wav*np.array(mask), color='c', label='keep')\n",
    "  ax[0,1].plot(y_mean, color='m', label='envelope')\n",
    "  ax[0,1].grid(False)\n",
    "  ax[0,1].set_title('Effect of the filter')\n",
    "  ax[0,1].set_xlabel('Sample Rate points')\n",
    "  ax[0,1].set_ylabel('Amplitude (norm)')\n",
    "  ax[0,1].legend(loc='best')\n",
    "  ax[1,1].axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  print('Listen to no sielence file')\n",
    "  write('test.wav', 16000, wav[mask])\n",
    "  display(Audio(data = 'test.wav', rate = 16000))\n",
    "\n",
    "  return wav[mask]\n",
    "\n",
    "def split_data(df, duration = 5 , overlap = 1, sr = 16000, threshold = 0.1, w = 10):\n",
    "  ''' Split an audio file info fixed lenght wave tensors\n",
    "  df -> dataframe with a list of file locations\n",
    "  duration -> duration of teh resulting wave patches in seconds\n",
    "  threshold -> signal strenght to identify as noise\n",
    "  overlap -> overlap in seconds between the splits\n",
    "  w-> Sample rate/w provides the window used to identify teh noise threshold\n",
    "  '''\n",
    "  split_dataframe = pd.DataFrame(columns = ['wave','label'])\n",
    "  for row in tqdm(range(df.shape[0])):\n",
    "    x = librosa.core.load('train_audio/' + df.iloc[row].filename,sr = sr)\n",
    "    x1 = [x[0]/np.max(np.abs(x[0]),axis=0),x[1]]                    # we have to normalize the amplitutre so taht the thresshold makes sence for all entries\n",
    "    mask, y_mean = envelope(x1[0], sr, threshold=threshold,w = w)\n",
    "\n",
    "    wav = x1[0]\n",
    "    wav = wav[mask]\n",
    "\n",
    "    segments = math.ceil(len(wav)/(sr*duration))\n",
    "    if segments == 1:\n",
    "      while len(wav) != duration*sr:\n",
    "        wav = np.concatenate([wav[:],wav[:duration*sr-len(wav)]])\n",
    "      split_dataframe.loc[len(split_dataframe.index)] = [wav, df.iloc[row].primary_label]\n",
    "      continue\n",
    "    for i in range(segments):\n",
    "      if (i+1) < segments:\n",
    "        split_dataframe.loc[len(split_dataframe.index)] = [wav[i*duration*sr:(i+1)*duration*sr], df.iloc[row].primary_label]\n",
    "\n",
    "      else :\n",
    "        split_dataframe.loc[len(split_dataframe.index)] = \\\n",
    "         [np.concatenate([wav[i*duration*sr:],wav[:duration*sr-len(wav[i*duration*sr:])]]), df.iloc[row].primary_label]\n",
    "        if np.concatenate([wav[i*duration*sr:],wav[:duration*sr-len(wav[i*duration*sr:])]]).shape[0]!=duration*sr :\n",
    "          print(row)\n",
    "          a = input()\n",
    "  return split_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M10clfJfQUB-"
   },
   "source": [
    "# Bird Classifier\n",
    "\n",
    "[The Whole project with the data can be found here](https://drive.google.com/drive/folders/1YGw6GGCBEjsg3dFgiEruD7szzUpMcSVW?usp=sharing)\n",
    "\n",
    "This project focuses on different ways to perform birdsong classification. The dataset used is from Kaggle [BirdCLEF 2023](#https://www.kaggle.com/competitions/birdclef-2023). As this project is mainly for learning purposses I have only considered 11 of the bird species. The names of the folders with audio recordings represent those species:<br>\n",
    "afpfly1 <br>\n",
    "bkctch1<br>\n",
    "cibwar1<br>\n",
    "grewoo2<br>\n",
    "laudov1<br>\n",
    "rindov<br>\n",
    "strher<br>\n",
    "varsun2<br>\n",
    "witswa1<br>\n",
    "yebapa1<br>\n",
    "yewgre1<br>\n",
    "<br>\n",
    "I have decided to use AST pre-trained model that is programed in Pytorch. I have decide to prepare a tensor with all the available data, that i can save and load on the CPU and then run the training on the GPU. In this document i will show how i am preparing the dat before sending it as input to the AST model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZVeqpdSjtPi"
   },
   "source": [
    "##Preparing the Train and Test datasets\n",
    "\n",
    "we will do the following actins:<br><br>\n",
    "###1. Loading the Records\n",
    "Load records for birds that have between 100 and 120 recoded files (those are 10 in total). Also we will add a bird with 5 recorded files to see how the model will handle a label with less examples(the names of the birds are mentioned above).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2156,
     "status": "ok",
     "timestamp": 1705414678519,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "sJYGXGki8-bv",
    "outputId": "1fb56a9b-f8c3-415b-963c-ab5981bfbc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "[Errno 2] No such file or directory: 'gdrive/MyDrive/Colab Notebooks/Birds2023'\n",
      "/content/gdrive/MyDrive/Colab Notebooks/Birds2023\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/gdrive')\n",
    "%cd gdrive/MyDrive/Colab Notebooks/Birds2023\n",
    "m_train = pd.read_csv('train_metadata.csv')\n",
    "df = m_train[['filename', 'primary_label']].copy()\n",
    "songs = list(df['primary_label'].value_counts()[(df['primary_label'].value_counts()>100) & (df['primary_label'].value_counts()<120)].index)\n",
    "songs.append(list(df['primary_label'].value_counts()[df['primary_label'].value_counts() == 5].index)[-1])\n",
    "df = df[df['primary_label'].isin(songs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnNrj0kunQaU"
   },
   "source": [
    "###2. Apply a filter to remove the silence in the recording\n",
    "An example is shown bellow.<br>You can hear the recording before and after the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915,
     "output_embedded_package_id": "1wBJOh_pvNxpQ0iDXpw85xpq4y3HUutfZ"
    },
    "executionInfo": {
     "elapsed": 13917,
     "status": "ok",
     "timestamp": 1705414699906,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "Xyc92WU4AOx_",
    "outputId": "5e0172a1-0ec1-4d85-8d3b-c913224fefdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(df.shape[0]):\n",
    "  x = librosa.core.load('train_audio/'+df.iloc[i].filename)\n",
    "  x1 = [x[0]/np.max(np.abs(x[0]),axis=0),x[1]]\n",
    "  test(x1, threshold = 0.125, w = 10)\n",
    "  print('Press \"s\" to stop or any letter to continue')\n",
    "  stop = input()\n",
    "  if stop == 's' : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Chqpy09ndMh"
   },
   "source": [
    "###3. Save the filtered data\n",
    "\n",
    "in the \"split_data\" line we load each file, resample it to 16000 samples in a second, apply the filter to it and store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42399,
     "status": "ok",
     "timestamp": 1707123313399,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "QVQ9nIYB-dlh",
    "outputId": "46400c76-6f2b-47c5-846a-a4caf47a8e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/MyDrive/Colab Notebooks/Birds2023\n"
     ]
    }
   ],
   "source": [
    "# new_df0 = split_data(df, threshold = 0.125)\n",
    "# new_df0.to_pickle('new_data.pkl')\n",
    "\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "%cd gdrive/MyDrive/Colab Notebooks/Birds2023\n",
    "new_df0 = pd.read_pickle('new_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lO8JK6anlz3"
   },
   "source": [
    "###4. Create Spectograms from the wave data\n",
    "we have our wave files and we should split them in a stratified fashion into train and test datasets. tHis is most easily done using train_test_split from the sllearn lybrary. this is not correct but as we will use other methods further in this project we will ease our efford for this one.<br>\n",
    "Then we will use ASTFeatureExtractor() to convert those wave patches into spectograms and save them into tensor files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1707123425844,
     "user": {
      "displayName": "Tomislav Vasilev",
      "userId": "01320317235636870252"
     },
     "user_tz": -120
    },
    "id": "2D1-VVgNeuD5",
    "outputId": "2cf3c48b-0d66-4bcf-a030-959e96962653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFB5sjSuaL4N"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df0.loc[:,'wave'], new_df0.loc[:,'label'], test_size = 0.25, stratify = new_df0.loc[:,'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGshREqWak0g"
   },
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor()\n",
    "dict_labels = dict(zip(y_train.unique(), range(len(y_train.unique()))))\n",
    "\n",
    "y_train = y_train.map(dict_labels, na_action='ignore')\n",
    "y_test = y_test.map(dict_labels, na_action='ignore')\n",
    "\n",
    "\n",
    "X_train = feature_extractor(np.array(X_train.tolist()), sampling_rate=16000, padding=\"max_length\", return_tensors=\"pt\").input_values\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.int64)\n",
    "\n",
    "\n",
    "X_test = feature_extractor(np.array(X_test.tolist()), sampling_rate=16000, padding=\"max_length\", return_tensors=\"pt\").input_values\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nI6o4tYoLB9"
   },
   "source": [
    "###5. We save the datasets for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybC1cWIoa-Ms"
   },
   "outputs": [],
   "source": [
    "torch.save(X_train, 'X_train_tensor.pt')\n",
    "torch.save(y_train, 'y_train_tensor.pt')\n",
    "torch.save(X_test, 'X_test_tensor.pt')\n",
    "torch.save(y_test, 'y_test_tensor.pt')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxZavt4XhRHN2EuV+rtG9z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
